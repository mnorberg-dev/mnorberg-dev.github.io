[{"content":"Over the past month, I’ve been deep in the weeds experimenting with Mosaic AI Gateway in Databricks — configuring endpoints, debugging integrations, and figuring out how all the pieces fit together. After plenty of trial and error, I wanted to share what I’ve learned so other developers can get up and running faster.\nIf you’ve explored Databricks’ newer AI features, you’ve probably noticed that the documentation, while thorough, can feel circular and fragmented. You often need to jump between Databricks docs, MLflow, and even OpenAI or Meta references, depending on your base model. It’s easy to lose the thread when trying to connect everything into a working system.\nThis post is my attempt to bridge those gaps with a concrete, end-to-end example — the kind I wish I’d had when I started.\nIn short: it’s a practical guide for developers who want to create a Databricks AI Gateway endpoint with tracing enabled, without getting lost in a maze of configuration steps and scattered documentation.\nWhat is Mosaic AI Gateway? Mosaic AI Gateway is Databricks’ unified interface for managing access to large language models (LLMs) and AI endpoints. It serves as a control layer between your applications and the models they call — helping teams standardize how they deploy, monitor, and secure their AI workloads.\nOut of the box, Mosaic AI Gateway provides several powerful features:\nPermission and rate limiting Payload logging Usage tracking AI Guardrails Fallbacks Traffic splitting Together, these capabilities make it easier to build production-ready AI systems that are safe, trackable, and cost-efficient.\nBut there’s one critical capability missing — and it’s the focus of this post: MLflow Tracing.\nIf you’re new to tracing, think of it as logging with context. A trace doesn’t just record the request and response; it also captures the intermediate steps and outputs along the way. When something breaks, those traces are invaluable for understanding what actually happened inside your model pipeline.\nSo the question becomes: how do you build a Mosaic AI Gateway endpoint that captures traces for each request?\nThat’s exactly the problem I set out to solve — creating a Databricks AI Gateway endpoint that generates a trace for every call.\nTo save you time, I’ll start by showing the solution up front. But the rest of this post takes a more narrative approach — walking through my learning process, the documentation trails I followed, and the false starts that eventually led to the right path.\nYou could argue that I should simply present the final setup and explain how it works. But I think it’s just as valuable to see what didn’t work. Sharing the failures, trade-offs, and dead ends helps demonstrate that the final approach isn’t just something that happened to work — it’s the one that stood up to testing and comparison.\nThat’s especially relevant today, when our first instinct to solve a problem is often to open ChatGPT or another LLM and ask for help. The response might look convincing, but is it the best approach? With fast-moving tools like Mosaic AI Gateway, the underlying models often haven’t been trained on the latest APIs or docs — meaning their answers can be incomplete, outdated, or just don\u0026rsquo;t work.\nBy sharing both the process and the result, I hope to give readers confidence that this solution is not only functional but well-researched and battle-tested — the one that held up after real experimentation, not just a lucky first try.\nThe Solution: ResponsesAgent As promised, let’s start with the answer.\nThe simplest way to enable tracing while still using all the features of Mosaic AI Gateway is to create and deploy a ResponsesAgent model in Databricks. This model type has MLflow Tracing enabled by default, and when you host it through the Gateway, you retain the same production capabilities — including rate limiting, logging, guardrails, and more.\nIn other words, a ResponsesAgent gives you the best of both worlds: full Gateway functionality and detailed trace data for every request.\nIf you’re just here for the implementation details, feel free to skip ahead to the end of this post — I walk through the setup step by step.\nBut if you’re curious about how I arrived at this solution, stick around. The next few sections cover the other approaches I tried, the dead ends I hit, and the reasoning that ultimately led to the ResponsesAgent path.\nAttempt 1: Foundational Models Like anyone learning a new system, I started at the beginning — the Mosaic AI Gateway Introduction page. It includes a table showing which features each model type supports:\nAt first glance, external model endpoints seemed the most capable. However, for this walkthrough, I focused on foundational models. They’re easier for readers to follow along with since they don’t require setting up authentication or access to external services.\nAside from that authentication step, foundational and external models function almost identically in configuration, serving behavior, and available Gateway features.\nAs a newcomer, I assumed I could host a foundational model and still get traces. That would have been ideal — use a proven model and get observability for free. So my first goal was to spin up an endpoint and confirm whether tracing worked out of the box.\nThe next section walks through how to create such an endpoint right up until the point where you learn that it does not support tracing. If you’re just here for the tracing solution, feel free to skip ahead to the Wrapper section — otherwise, here’s what I found.\nFoundational Model Endpoint Creation When creating infrastructure in Databricks, there are usually multiple paths to the same result — Terraform, Python, SQL, or the UI. For investigative work like this, I prefer the UI. It’s quick to explore configurations and verify behavior visually, even though in production you’d typically automate the process.\nTo create an endpoint, go to Serving → Create Serving Endpoint, then choose Foundational Models in the Served Entities section. This opens the endpoint creation menu shown below. Working through it from top to bottom, first give your endpoint a name, then configure the Served Entities section.\nThe Served Entities section is where you select the model you want to serve. Click on the \u0026lsquo;Select an Entity\u0026rsquo; text in the \u0026lsquo;Entity details\u0026rsquo; box. Then choose the \u0026lsquo;Foundational Models\u0026rsquo; option from the radio list.\nAfter selecting that option, a second pop-up lists both foundational and external models, as shown below.\nThis can be confusing at first because the pop-up menu is labeled Foundational Models, yet it also lists external providers. I’m calling this out for two reasons:\nIf you’re following along and want to use an external model instead, this is how you’d do it — select your model, complete the additional authentication step, and you’ll have an external model endpoint. You could then substitute that model’s name wherever a foundational model is used in this post.\nIt highlights that the only real difference between foundational and external model endpoints is authentication; otherwise, the setup is identical.\nOnce you’ve chosen a foundational model — in this case, I selected GPT OSS 20B — you’ll see the configuration screen below.\nHere you can configure throughput and scaling options, but notice there’s no tracing toggle. So how do you set up tracing for your model?\nNote: When I first started, I saw some models with a tracing toggle in the UI, but those seem to have disappeared. With Databricks’ rapid feature updates, I’ve seen frequent changes in the platform which often impact what you\u0026rsquo;re working.\nWhen I first planned this post, I expected to ask, “What do you do if your model doesn’t support tracing?” — but now, none of them do. Fortunately, I still have an answer.\nSearching for the Missing Piece Not knowing how, I dug deeper into the documentation. I found plenty of useful material about tracing GenAI apps, but very few that answered the core question: “How do I set up an endpoint that generates traces by default?”\nHere are a few of the more relevant but incomplete resources:\n\u0026ldquo;Get started: MLflow Tracing for GenAI (Databricks Notebook)\u0026rdquo; \u0026ndash; great for learning how traces work, but only covers tracing single notebook requests. \u0026ldquo;Tracing OpenAI\u0026rdquo; — shows how to trace OpenAI calls, but not how to create an endpoint with tracing enabled. As you\u0026rsquo;ll find if you start to go through the docs as well, most examples show how to trace one request, not how to create an endpoint creates a trace for each request it recieves.\nFinally, I stumbled on this artice \u0026ldquo;Deploy agents with tracing\u0026rdquo;. This article finally pointed me in the right direction — though I was skeptical at first. My initial thought was: “Why do we need an agent? This seems like such a simple thing to achieve — an agent feels overcomplicated.”\nBut that article sparked an idea: what if I could create a wrapper model that calls the underlying model while automatically handling tracing? That realization set the direction for my next experiment.\nAttempt 2: Custom Python Model If foundational models couldn’t generate traces directly, then I needed to create something that could. The answer was to build a wrapper — a lightweight layer that calls the real model behind the scenes while also handling tracing. Essentially, requests come into the wrapper, the wrapper calls the underlying model (like GPT OSS 20B), and then it returns the response unchanged. The difference is that the wrapper can log traces automatically.\nThe Wrapper Approach At this point, we knew that while Databricks AI Gateway could host a foundational model directly, it didn’t support tracing in that configuration. Creating a wrapper model will allow us to use our foundational model and attain tracing functionality.\nHere’s the high-level plan:\nBuild a small model class that wraps around our foundational model. Configure the class so that tracing is enabled by default. Register this model in Unity Catalog. Deploy it as a Serving Endpoint, just like before — except now the endpoint will support both AI Gateway and tracing. This approach gives us the same Gateway functionality as before, but with complete trace visibility. If this sounds confusing, just wait till we get to the code examples which I think will help clarify the setup.\nWrapper Options Once I knew I needed a wrapper, the next question was how to define it with MLflow.\nThere are two main ways to do this:\nCustom Python Model – You define your own PythonModel class and control every detail of how predictions are made. Responses Agent Model – You use one of Databricks’ prebuilt agent types (like ResponsesAgent) that already integrate tracing, tool use, and prompt orchestration. As I mentioned before, I was skeptical of the responses agent solution. My goal wasn’t to build a full agent-based system — I just wanted to trace model calls. That left me with one clear option: a Custom Python Model.\nThat said, the Gen AI Apps guide clearly recommends using response agents over custom python models. However, I still wasn\u0026rsquo;t convinced. I wanted somthing simple, transparent, and easy to debug.\nSo I started by building the Python model — and as you can probably guess, it worked, but not as well as I’d hoped. Once I had it running, I compared it to a ResponsesAgent implementation and found that the agent approach was cleaner and better aligned with Databricks’ newer OpenAI Responses API. It also seemed more future-proof as Databricks expands its generative AI tooling.\nImplementing a Custom Python Model Creating a custom Python model in MLflow involves encapsulating your logic in a class that inherits from mlflow.pyfunc.PythonModel. The key method is predict, which receives input and returns the model’s response.\nSince this model is only a wrapper, our predict method just forwards the request to the underlying foundational model and returns its response.\nIf you’d like to see the relevant documentation, here are the key references I used:\nMLflow Python Model Guide Custom Serving Applications MLflow Python Model Class 1. Install dependencies\nIn the first notebook cell, install the following libraries using the code below:\n%pip install -U -qqqq databricks-openai dbutils.library.restartPython() In addition to installing the databricks-openai package, this command upgrades MLflow — which is important because the current default serverless environment typically includes MLflow Skinny 2.x, but the code below requires MLflow 3.x.\nDefault Libraries Installed Libraries mlflow-skinny==2.21.3 mlflow==3.4.0 databricks-connect==16.4.2 mlflow-skinny==3.4.0 databricks-sdk==0.49.0 mlflow-tracing==3.4.0 databricks-ai-bridge==0.8.0 databricks-connect==16.4.2 databricks-openai==0.6.1 databricks-sdk==0.49.0 databricks_vectorsearch==0.59 ⚠️ Note: MLflow’s documentation warns against having both mlflow and mlflow-skinny installed simultaneously. I haven’t encountered issues with this setup, and several Databricks examples use the same install pattern — so I decided to trust it. Still, it’s something to watch for in case anything breaks or behaves unexpectedly.\n2. Define your model\nHere’s the cell that defines the model and writes it to a file called model.py. Let’s walk through the file from top to bottom.\n%%writefile model.py import mlflow from mlflow.pyfunc import PythonModel, PythonModelContext from databricks.sdk import WorkspaceClient from typing import Any, Dict, List, Optional class ModelWrapper(PythonModel): def __init__(self): self.client = WorkspaceClient().serving_endpoints.get_open_ai_client() def predict( # pyright: ignore self, context: PythonModelContext, model_input: List[Dict[str, str]], params: Optional[Dict[str, Any]] = None, ) -\u0026gt; List[str]: results = [] response = self.client.chat.completions.create( model=\u0026#34;databricks-meta-llama-3-1-8b-instruct\u0026#34;, messages=model_input ) results.append(response.choices[0].message.content) return results mlflow.openai.autolog() mlflow.set_tracking_uri(\u0026#34;databricks\u0026#34;) mlflow.set_experiment(\u0026#34;/Shared/mn-demo-experiments\u0026#34;) mlflow.models.set_model(ModelWrapper()) The %%writefile command writes this cell’s contents to a file — in this case, model.py. This is required because the model registration step later references this file directly. It also keeps all the code for this example self-contained within a single notebook, aside from a small requirements.txt file.\nThe ModelWrapper class inherits from PythonModel. In the constructor, we initialize a Databricks WorkspaceClient to access the serving endpoint client that will call our base model.\nIf you’d like to connect to an external model instead of a foundational one, here’s how:\nFollow the steps in Attempt 1 to create a serving endpoint for your external model (e.g., external-model-endpoint). In the code above, replace the model parameter in the chat.completions.create call with the name of your external model. That’s it — your wrapper will now route requests to your external model instead. The next portion of the file is the predict method, which defines the inference logic — sending the request to the model and returning its response. You’ll also notice that I added type hints to this function. This prevents MLflow from emitting log warnings during model creation.\nThe last four lines at the bottom of the cell are critical:\nmlflow.openai.autolog() mlflow.set_tracking_uri(\u0026#34;databricks\u0026#34;) mlflow.set_experiment(\u0026#34;/Shared/mn-demo-experiments\u0026#34;) mlflow.models.set_model(ModelWrapper()) These lines enable tracing and logging.\nmlflow.openai.autolog() enables detailed trace collection. Without it, you’d only get partial trace data through decorators. The next two lines (set_tracking_uri and set_experiment) tell MLflow where to store trace data in Databricks. If you omit the tracking URI and experiment setup, traces will only appear when you call the endpoint interactively from a Databricks notebook — not when hitting it via API. Finally, set_model() sets the model object that is going to be logged. 3. Register the model\nOnce your model.py file is defined, the final step is to register it in Unity Catalog. The code below handles that process:\nimport mlflow from mlflow.models.resources import DatabricksServingEndpoint example = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the fibonacci sequence\u0026#34;}, ] with mlflow.start_run(): mlflow.pyfunc.log_model( name=\u0026#34;mn-ai-demo\u0026#34;, python_model=\u0026#34;model.py\u0026#34;, input_example=example, registered_model_name=\u0026#34;workspace.default.mn-ai-demo\u0026#34;, pip_requirements=\u0026#34;requirements.txt\u0026#34;, resources=[DatabricksServingEndpoint(endpoint_name=\u0026#34;databricks-gpt-oss-20b\u0026#34;)], ) Since the %%writefile command in the previous cell writes your model code to disk rather than keeping it in memory, you’ll need to re-import MLflow (and any other dependencies) before running this step.\nThe example variable defines a simple test input that MLflow uses to validate your model. Inside your requirements.txt, include databricks-openai — this package is required for deployment to work correctly.\nWhat to Expect When You Run Model Registration Code\nWhen you execute the registration cell, MLflow confirms a successful run and model version creation. The example input and response will appear inline:\nImmediately below that, you’ll see your first trace — showing not only the input and output, but detailed metadata such as call hierarchy, timing, and context:\nAt this point, you can also test your model directly from a notebook without importing the model.py file. Simply call mlflow.pyfunc.load_model() to load it from Unity Catalog, then invoke predict(). Due to the way we defined our model, you each notebook inference will automatically generate a trace, as shown here:\nCreating the Endpoint With your model registered, the next step is deployment.\nNavigate back to the Serving page, click Create Endpoint, and select your new Python model. You’ll now see an option to enable tracing — it should already be turned on by default:\nScroll down to the AI Gateway section to configure settings like the Inference Table, which logs all requests and responses (though not as richly as traces). Note that inference tables aren’t yet available in the free Databricks tier.\nOnce your endpoint finishes deploying, the interface should show it as Active and ready for REST requests:\nYou can now test it using curl or your preferred REST client — I’ve been using the REST Client extension in VS Code. After sending a few requests, check your Experiments page under the shared experiment path — you’ll see fresh traces appear there:\nAt this point, you’ve successfully built an endpoint with full Mosaic AI Gateway functionality and detailed tracing — all through a custom Python model. You\u0026rsquo;re probably wondering why I am not recommending the python model. The issue is that I had a difficult time setting up streaming responses with the custom python model. If streaming had worked seamlessly, this might have been my final recommendation. But it didn’t — and that’s what led me to explore the ResponsesAgent next.\nWhat Went Wrong with Streaming Requests If you’ve looked at the documentation I referenced earlier, you may have noticed that the PythonModel class in MLflow also defines a predict_stream function that you can override to support streaming requests.\nHere’s the basic idea: we implement the predict_stream function inside our model class. When a REST request includes a streaming parameter, MLflow will call predict_stream instead of predict, allowing the model to return partial results as they arrive.\nHowever, this is where things start to get tricky. The predict_stream function is designed to take the same PyFunc-compatible input as predict, so I defined it as follows:\ndef predict_stream( # pyright: ignore self, context: PythonModelContext, model_input: List[Dict[str, str]], params: Optional[Dict[str, Any]] = None, ) -\u0026gt; Iterator[str]: response = self.client.chat.completions.create( model=\u0026#34;databricks-meta-llama-3-1-8b-instruct\u0026#34;, messages=model_input, stream=True, ) full_message = \u0026#34;\u0026#34; for chunk in response: if chunk.choices and chunk.choices[0].delta.content: new_content = chunk.choices[0].delta.content full_message += new_content yield new_content yield full_message Testing in a Notebook\nWhen I imported the module directly and invoked predict_stream, it worked perfectly — chunked responses arrived in real time, and the trace appeared in the notebook output.\nThis confirmed that the function worked when called directly from the notebook. I could see all the chunks streaming back one by one, and tracing behaved exactly as expected.\nEncouraged by this, I decided to test the same functionality through other methods — and that’s where things started to fall apart.\nTesting Through the Loaded Model and REST API\nAfter registering the model in Unity Catalog, I tried invoking it again by loading it with mlflow.pyfunc.load_model() and calling predict_stream. This time, it failed.\nYou can see in the image above that predict() still works fine when the model is loaded this way — but predict_stream() doesn’t. The same issue appears when invoking it through a REST endpoint:\nAt this point, I was puzzled. The function clearly worked in one context but failed in another. I briefly considered adding a streaming flag to the predict method itself (e.g., predict(streaming=True)), but that felt like a hack and went against the intended design of the MLflow API. It also wasn’t as clean as maintaining two clearly defined methods — one for standard predictions and one for streaming.\nSo I started digging into the root cause.\nDigging into the Cause\nWhy didn’t predict_stream work when the model was loaded from Unity Catalog?\nThe key detial lies in what mlflow.pyfunc.load_model() actually returns. According to the MLflow docs, it doesn’t return your PythonModel directly — it returns a PyFuncModel, a wrapper class that standardizes how models are called.\nWhen you invoke predict_stream() on the loaded model, you’re actually calling the wrapper’s version of that function, which then delegates to your implementation. Unfortunately, something in that handoff — specifically in how inputs are validated and passed through — seems incompatible with the OpenAI-style message list I was using.\nFor anyone interested in exploring further, you can inspect the predict_stream implementation in the PyFuncModel source code.\nWhat frustrated me about this experience is that MLFlow PythonModel documentation states that both predict and predict_stream accept PyFunc-compatible input. Since my input worked perfectly with predict, I expected it to work with predict_stream as well.\nThe Inference API docs also claim that “a list of any type” should be valid input — further suggesting this should have worked.\nWhere Things Stand\nTo make predict_stream work, I had two main options:\n(1) change its input format, or (2) modify predict so it could handle streaming requests as well.\nBoth felt like poor tradeoffs. I didn’t want to maintain separate input schemas for predict and predict_stream, and I also didn’t like the idea of adding a “streaming” flag to predict just to make it behave differently.\nIf you don’t need streaming, the custom Python model approach is still an excellent choice — it’s flexible, powerful, and integrates seamlessly with Databricks. But for my use case — supporting both standard and streaming completions — it wasn’t enough.\nSo it was time to move on to Attempt 3: the ResponsesAgent.\nOption 3: Responses Agent Due the difficulties I had in implementing streaming support with the custom python model, I decided to try the responses agent to see for myself if it was good or not.\nIn the databricks documentation, I found a \u0026ldquo;simple\u0026rdquo; guide that shows how to create a responses agent endpoint. This was a good starting point for me, but I didn\u0026rsquo;t love the databricks code. In fact, I thought there were several parts of the notebook that had bad programming practices and could be simplified.\nMention the call stack\nMention the libraries\nHowever, this still proved to be a good example that I used to modify into a solution that I thought was a little simpler for a new users like myself.\nImplementing Responses Agent As before, our notebook needs to start with an installation cell to make sure we have the required libraries and to update the mlflow base version from 2 to 3. This time however, we also need to install the databricks-agents library. So our installation cell now becomes:\n%pip install -U -qqqq databricks-openai databricks-agents dbutils.library.restartPython() Similarly, our next cell in the notebook is going to define the agent.\nimport mlflow from mlflow.pyfunc import ResponsesAgent from mlflow.types.responses import ( ResponsesAgentRequest, ResponsesAgentResponse, ResponsesAgentStreamEvent, ) from databricks.sdk import WorkspaceClient from typing import Generator class SimpleResponsesAgent(ResponsesAgent): # pyright:ignore def __init__(self): self.workspace_client = WorkspaceClient() self.client = self.workspace_client.serving_endpoints.get_open_ai_client() self.model = \u0026#34;databricks-gpt-oss-20b\u0026#34; def predict(self, request: ResponsesAgentRequest) -\u0026gt; ResponsesAgentResponse: messages = request.input response = self.client.chat.completions.create( model=self.model, messages=self.prep_msgs_for_cc_llm(messages), ) return ResponsesAgentResponse( output=[ self.create_text_output_item( text=response.choices[0].message.content, id=response.id ) ], ) def predict_stream( self, request: ResponsesAgentRequest ) -\u0026gt; Generator[ResponsesAgentStreamEvent, None, None]: response = self.client.chat.completions.create( model=self.model, messages=self.prep_msgs_for_cc_llm(request.input), stream=True, ) item_id = 1 full_message = \u0026#34;\u0026#34; for chunk in response: if chunk.choices and chunk.choices[0].delta.content: new_content = chunk.choices[0].delta.content full_message += new_content yield ResponsesAgentStreamEvent( **self.create_text_delta( delta=new_content, item_id=f\u0026#34;msg_{item_id}\u0026#34; ), ) item_id += 1 yield ResponsesAgentStreamEvent( type=\u0026#34;response.output_item.done\u0026#34;, item=self.create_text_output_item( # pyright:ignore text=full_message, id=f\u0026#34;msg_{item_id-1}\u0026#34;, ), ) return mlflow.openai.autolog() mlflow.set_tracking_uri(\u0026#34;databricks\u0026#34;) mlflow.set_experiment(\u0026#34;/Shared/mn-demo-experiments-agent\u0026#34;) mlflow.models.set_model(SimpleResponsesAgent()) There are some key differences to point out in this iteration versus the last one. First, our wrapper class now inherits from the MLflow Responses agent class, not the python model class. The next thing to point out is the call to chat completions which I am going to put again below.\nmessages = request.input response = self.client.chat.completions.create( model=self.model, messages=self.prep_msgs_for_cc_llm(messages), ) This is pretty similar to the call to chat completions earlier. But what heck is the schema of the request parameter? What does this prep_msgs_for_cc_llm do? It\u0026rsquo;s not actually defined in the code.\nThese are all questions I hada when I first started working with this code and things I would like to point out.\nSo first, the request is of type ResposnesAgentRequest. Here is an example of these schemas from the official Mlflow docs. You can find the link to this example here: https://mlflow.org/docs/latest/genai/serving/responses-agent/#schema-and-types and some more information here: https://mlflow.org/docs/latest/api_reference/python_api/mlflow.types.html#mlflow.types.responses.ResponsesAgentRequest.\n# Example Request schema { \u0026#34;input\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the weather like in Boston today?\u0026#34;, } ], \u0026#34;tools\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: {\u0026#34;location\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}}, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;unit\u0026#34;], }, } ], } # Example Response schema { \u0026#34;output\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;some-id\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;output_text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;rainy\u0026#34;, } ], } ], } There are some caveats and interesting things that happen when you invoke this function. All of which I\u0026rsquo;ll explain later.\nOne key thing to notice is that this input is actually the input for the open ai responses api: https://platform.openai.com/docs/api-reference/responses/create. Why is this important? Well, we are not calling the open ai responses api, we are using the chat completions api. It\u0026rsquo;s all importatnt to note that the chat completions api expects the input in a different schema than the responses api.\nSo what\u0026rsquo;s going on here? Well, you can think of the resposnes agent class as being built around the Responses API from open ai, not the chat completions api. This new api is their answer to the new agentic models and it supposed to improve blah blah blah.\nSo why not just do this instead of the call the chat completions api?\nmessages = request.input response = client.responses.create( model=self.model, messages=messages, ) Should work right? Wrong, it does not work yet in databricks with our setup. So here\u0026rsquo;s my theory on this:\nWhen we created our client which is able to access the model, we did so with this code.\nfrom databricks.sdk import WorkspaceClient client = WorkspaceClient().workspace_client.serving_endpoints.get_open_ai_client() This allows us to get a client that can access models which are registered in databricks, wether or not they are hosted in databricks. The responses api is so new, my guess is that when you create a client using the WorkspaceClient library, databricks has not updated their code yet to accomodate the responses api. Thus you get an error whenever you call the responses api, even with simple requests.\nThis theory of mine is further supported by all the example notebooks currently floating around the databricks docs. In each one I have seen, the client is created using the code above, the base class inherits from responses agent which means the input is tailored to the responses api, but they don\u0026rsquo;t use the resposnes api.\nA side note is that there is another way to call the responses api in databricks. You initialize your client like this.\nfrom openai import OpenAI client = OpenAI() This method would work, but you also have to set up the correct environment variables to authenticate. The databricks sdk method does not require this since you are hitting models registered in databricks, whereas the other method with the openai library, this is not the case.\nSo this brings us back to the chat copmletions call\nmessages = request.input response = self.client.chat.completions.create( model=self.model, messages=self.prep_msgs_for_cc_llm(messages), ) This will not work because messages is in the wrong format. To get around this, the response agent class has a method called prep_msgs_for_cc_llm which actually stands for prepare messages for chat completion llm, in case you did not put it together. I think of this method as a translater. It takes in messages for the responses api and transforms it to messages for the chat completions api.\nNow finally, we have the return statement. What\u0026rsquo;s going on there?\nreturn ResponsesAgentResponse( output=[ self.create_text_output_item( text=response.choices[0].message.content, id=\u0026#34;msg_1\u0026#34; ) ], ) Well I mentioned before, the responses agent is tailored to the responses api, not the chat completions api. Our response variable though has a schema matching the chat completions response and we need to return a ResponsesAgentResponse type. So we use the constructor to create a proper response object. We wrap the text in another helper function from the ResponsesAgent class to create a text output. To see the list of outputs: see here https://mlflow.org/docs/latest/genai/serving/responses-agent/#creating-agent-output.\nAlso, don\u0026rsquo;t worry about only returning the text. You may be wondering, there are a bunch of details in the response variable, not just the content. We don\u0026rsquo;t want to lose though. That\u0026rsquo;s a good point, you don\u0026rsquo;t want to lose those, but you don\u0026rsquo;t necessarily want to return them to the caller either. Not to fear, the traces capture these details by default!\nWhat about the streaming?\nWell, with responses agent, we did not have the same issues that we had with the python model. The setup worked much smoother.\nHow does the code work?\nWell we start by passing a streaming parameter to the model to indicate we want to stream the result.\nIn streaming, the response comes back in chunks. So you as the developer need to accumulate the chunks into the final message. In my code above, you can see those accumulation steps. As the code accumulates the chunks, we yield stream events to show all the deltas (changes) to the user. Finally, we yield an even to illustrate that we are done to the user.\nThere are two more cells to the agent code which I\u0026rsquo;ll put here to talk about:\n# cell 1 import mlflow from mlflow.models.resources import DatabricksServingEndpoint UC_LOCATION = f\u0026#34;workspace.default.agent-demo-endpoint\u0026#34; mlflow.openai.autolog() with mlflow.start_run(): logged_agent_info = mlflow.pyfunc.log_model( name=\u0026#34;agent-demo-endpoint\u0026#34;, python_model=\u0026#34;agent_model_streaming.py\u0026#34;, registered_model_name=UC_LOCATION, pip_requirements=\u0026#34;requirements.txt\u0026#34;, resources=[DatabricksServingEndpoint(endpoint_name=\u0026#34;databricks-gpt-oss-20b\u0026#34;)], ) # COMMAND ---------- from databricks import agents # pyright: ignore agents.deploy( UC_LOCATION, model_version=logged_agent_info.registered_model_version, ) The first cell logs the model. You should be familiar with this by now. The second cell will actually deploy the agent for you so you don\u0026rsquo;t have to go the serving ui to do this manually. If the model is already deployed, it will apply updates for you so you don\u0026rsquo;t have to do it by hand which is really cool.\nConclusion It was a long journey to get all the way to responses agents and feel good about the journey. For those that read the whole article, you can see why the newcomer would start at foundational models, then might navigate to custom python models before finally landing at responses agents.\nI hope that in presenting these steps, you come away with a few things\nYou learned a lot about mosaic ai gateway, model serving, python models, and responses agents If you are looking to create somethign similar, you start right at responses agents and feel confident it is a good choice for you. That the other options were experimented by someone and you dont need to research it again. Thank you for all those that read the whole thing. I hope you enjoyed it. As I was writing, I thought to myself, my god this is really long. But I wanted it to be thorough and cover all the cases so that I answer the questions I had when I started.\nIf you enjoyed this, please be on the lookout for more articles regarding data engineering, ai, and databricks.\n","permalink":"http://localhost:1313/posts/ai-gateway/","summary":"\u003cp\u003eOver the past month, I’ve been deep in the weeds experimenting with Mosaic AI Gateway in Databricks — configuring endpoints, debugging integrations, and figuring out how all the pieces fit together. After plenty of trial and error, I wanted to share what I’ve learned so other developers can get up and running faster.\u003c/p\u003e\n\u003cp\u003eIf you’ve explored Databricks’ newer AI features, you’ve probably noticed that the documentation, while thorough, can feel circular and fragmented. You often need to jump between Databricks docs, MLflow, and even OpenAI or Meta references, depending on your base model. It’s easy to lose the thread when trying to connect everything into a working system.\u003c/p\u003e","title":"Databricks AI Gateway Guide"},{"content":"Disclaimer: This presentation was originally created when the technology was called Delta Live Tables (DLT). Databricks has since rebranded it as Lakeflow Declarative Pipelines. While the name has changed, many of the strategies and techniques for applying data quality rules remain the same. For the latest documentation, see the Databricks Lakeflow Declarative Pipelines docs.\nEnsuring high data quality is critical for analytics, decision-making, and building reliable AI/ML models. Without it, organizations risk costly errors, unreliable insights, and ineffective models.\nIn this talk, I share a practical guide to implementing data quality expectations within Databricks’ Delta Live Tables (DLT). Think of expectations as “unit tests for data” — rules that define what your data should look like. By applying them, you can:\nMeasure the proportion of data that meets quality standards Quarantine or block bad data before it flows downstream Detect bugs in code that cause quality issues Lay the foundation for data quality monitoring and reporting The session walks through a four-step process for writing expectations: profiling your data, collaborating with domain experts, documenting rules, and translating them into SQL. I also share lessons learned, tips for managing quarantines, and strategies to balance strict vs. loose expectations.\n📺 Watch the full presentation on YouTube https://www.youtube.com/watch?v=Uk3kN97NgPk\u0026amp;t=2s\n📑 Download the slides and demo code on GitHub https://github.com/mnorberg-dev/data-expectations\nWhether you’re a developer working hands-on with DLT pipelines or simply looking to understand how to raise the bar on your organization’s data quality, this talk will give you actionable tools to get started.\n","permalink":"http://localhost:1313/posts/data-quality-expectations/","summary":"\u003cp\u003e\u003cstrong\u003eDisclaimer:\u003c/strong\u003e This presentation was originally created when the technology was called \u003cstrong\u003eDelta Live Tables (DLT)\u003c/strong\u003e. Databricks has since rebranded it as \u003cstrong\u003eLakeflow Declarative Pipelines\u003c/strong\u003e. While the name has changed, many of the strategies and techniques for applying data quality rules remain the same. For the latest documentation, see the \u003ca href=\"https://docs.databricks.com/aws/en/dlt/\"\u003eDatabricks Lakeflow Declarative Pipelines docs\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eEnsuring high data quality is critical for analytics, decision-making, and building reliable AI/ML models. Without it, organizations risk costly errors, unreliable insights, and ineffective models.\u003c/p\u003e","title":"Databricks Data Quality Expectations Guide"},{"content":"Databricks is quickly becoming one of the most popular data lakehouse platforms out there. Its popularity is growing fast, and for many developers, a significant portion of the job happens directly in the browser inside their Databricks workspace.\nYes, I know—there are popular extensions that let you develop code outside of Databricks. Many folks swear by the VS Code extension because they prefer working in their favorite editor. But at the end of the day, you’re still going to spend time in the browser making sure your code runs as expected.\n👉 If you’d rather skip ahead and get straight to the solution, I’ve published the script (with setup instructions) in this GitHub repo.\nThe Problem: Too Many Tabs, Too Few Clues Many organizations using Databricks have multiple workspaces to represent different environments—think dev, qa, and prod. Some even have additional splits like staging or sandbox.\nIf you’re working across all of them, here’s the problem: your browser tabs all look the same.\nNote: URLs in images are redacted, but in your environment, the redacted portions will appear as GUIDs.\nInside the workspace, the situation isn’t much better. It\u0026rsquo;s not easy to determine which environment you are in. Technically, each environment does identify itself in two places though:\nThe domain name. Every environment has a URL like adb-\u0026lt;long string of numbers\u0026gt;. But let’s be honest—nobody remembers arbitrary GUIDs. It’s the same reason DNS exists: humans prefer names like google.com instead of memorizing IP addresses. A small piece of text in the top-right corner. Sure, it’s there, but after seeing it hundreds of times, your brain starts ignoring it. It’s easy to miss, and if you’re juggling multiple tabs, you can’t even see that text without clicking into each one. The end result? Confusion, context-switching, and the very real risk of running a query in prod that you meant for dev.\nBut Wait, Isn’t There a Color Trick? Some people solve this by assigning different themes—say, dark mode for prod and light mode for dev. Clever idea, but it falls apart quickly:\nThere are only two color schemes available and many organizations have three workspaces meaning one environment will inevitably match another. If two or more environments share a metastore, changing one changes the other. And let’s be honest: some developers simply refuse to use a theme they don’t like. So while the color trick can work in a pinch, it’s not a real solution.\nTo activate dark mode, navigate to Settings → User Preferences → “Prefer Dark.”\nIf you don’t enable this setting, your environment theme will default to match the light theme shown in the images above.\nThe Solution: Tampermonkey to the Rescue Enter Tampermonkey, a browser extension that lets you run custom JavaScript whenever you visit a matching URL.\nWith a short script, you can automatically label your Databricks browser tabs with an emoji + text identifier for each environment. Suddenly, dev, qa, and prod are crystal clear.\nIt’s simple, lightweight, and makes a huge difference:\nFaster navigation between environments Reduced risk of editing the wrong workspace A quality-of-life boost you’ll wonder how you lived without And best of all: little to no performance impact. I’ve been running this script for weeks, and my browser hasn’t skipped a beat. Below, I\u0026rsquo;ve provided an image illustrating what your dev environment will look like after setting up the Tamper Monkey script for yourself.\nHow to Set It Up Getting your Databricks tabs labeled automatically is quick and easy. You’ll need to:\nInstall Tampermonkey Create a new script Paste in the code Configure your environment domains Save and enable the script Enable user scripts in your browser (Chrome only) Verify it’s working Note: These instructions are written for Google Chrome, but the steps can be adapted to other browsers that support Tampermonkey.\nFollow the steps below.\n1. Install Tampermonkey Download Tampermonkey for your browser from tampermonkey.net. Restart your browser after installation. 2. Create a New Script Open the Tampermonkey extension → Dashboard → “Create a new script.” Delete the boilerplate code so the editor is blank. 3. Add the Script Copy and paste the following code into the new script:\n// ==UserScript== // @name Databricks Tab Emoji Label // @namespace http://tampermonkey.net/ // @version 1.5 // @description Add emoji and label to Databricks tab title based on environment // @author Matthew Norberg // @match https://\u0026lt;your-domain-here\u0026gt;.azuredatabricks.net/* // @grant none // ==/UserScript== (function() { \u0026#39;use strict\u0026#39;; function updateTitle() { const url = window.location.href; let label = \u0026#39;\u0026#39;; let emoji = \u0026#39;\u0026#39;; // Define your environment domains here let devDomain = \u0026#39;\u0026#39;; let qaDomain = \u0026#39;\u0026#39;; let prodDomain = \u0026#39;\u0026#39;; if (url.includes(devDomain)) { label = \u0026#39;DEV\u0026#39;; emoji = \u0026#39;🟢\u0026#39;; } else if (url.includes(qaDomain)) { label = \u0026#39;QA\u0026#39;; emoji = \u0026#39;🟡\u0026#39;; } else if (url.includes(prodDomain)) { label = \u0026#39;PROD\u0026#39;; emoji = \u0026#39;🔴\u0026#39;; } else { label = \u0026#39;OTHER\u0026#39;; emoji = \u0026#39;⚪\u0026#39;; } if (!document.title.startsWith(`[${emoji} ${label}]`)) { document.title = `[${emoji} ${label}] ${document.title}`; } } // Initial run setTimeout(updateTitle, 3000); // Monitor for URL changes in SPA let lastUrl = location.href; new MutationObserver(() =\u0026gt; { const currentUrl = location.href; if (currentUrl !== lastUrl) { lastUrl = currentUrl; setTimeout(updateTitle, 2000); } }).observe(document, {subtree: true, childList: true}); })(); 4. Configure Environment Domains Near the top of the script, replace the placeholders with your actual environment domains:\nlet devDomain = \u0026#39;your-dev-domain\u0026#39;; let qaDomain = \u0026#39;your-qa-domain\u0026#39;; let prodDomain = \u0026#39;your-prod-domain\u0026#39;; 5. Save and Enable the Script Save your changes (File → Save).\nMake sure the script toggle in Tampermonkey is turned on (green).\n6. Enable User Scripts in Chrome Go to chrome://extensions → find Tampermonkey → click Details.\nMake sure Allow user scripts is enabled.\nNote: Other browsers may have different settings for allowing user scripts—adapt accordingly.\n7. Verify installation Open your Databricks environment in a new tab.\nAfter a few seconds, your tab title should display the correct emoji + environment label.\nTroubleshooting Tab title doesn’t update right away\nThe script intentionally waits 3 seconds after page load before applying changes.\nWithout this delay, the title gets updated but is overwritten during the web page loading process.\nIn testing, 3 seconds worked well, but you can adjust this by editing the setTimeout call in the code:\nsetTimeout(updateTitle, 3000); Still not working?\nDouble-check that:\nThe script is enabled in Tampermonkey. Your domains are correctly set in both the variables and the @match lines. “Allow user scripts” is enabled in your browser’s extension settings. Final Thoughts It’s a small tweak, but it solves a surprisingly big problem. If you’re juggling multiple Databricks environments, this little script will save you from confusion—and maybe even prevent a mistake or two.\nSometimes the best tools aren’t the big, complicated ones. They’re the tiny hacks that make your day smoother.\n👉 You can grab the full script and step-by-step setup instructions in the databricks-tools-repo here.\n","permalink":"http://localhost:1313/posts/databricks-tab-label-tool/","summary":"\u003cp\u003eDatabricks is quickly becoming one of the most popular data lakehouse platforms out there. Its popularity is growing fast, and for many developers, a significant portion of the job happens directly in the browser inside their Databricks workspace.\u003c/p\u003e\n\u003cp\u003eYes, I know—there are popular extensions that let you develop code outside of Databricks. Many folks swear by the VS Code extension because they prefer working in their favorite editor. But at the end of the day, you’re still going to spend time in the browser making sure your code runs as expected.\u003c/p\u003e","title":"The Databricks Tool You Didn't Know You Needed"},{"content":"Hi! I’m Matthew Norberg, a Data Engineer with a passion for turning complex data challenges into clean, maintainable, and high-performing solutions. Over the past several years, I’ve had the opportunity to work with Databricks, Azure, and a variety of modern data tools, building platforms, pipelines, and systems that help organizations make better use of their data.\nMy Journey in Data Engineering I’ve always had a passion for programming and problem-solving, which first led me down the path of software engineering during my undergraduate studies. In graduate school, I started working more with data, earning a degree in Computer Science with a concentration in Data Science. This combination of software engineering and data expertise naturally led me to data engineering—a perfect middle ground between building robust systems and working with meaningful data. My background in CS and data science has prepared me well to tackle the challenges of modern data engineering.\nMy approach to data engineering can be summed up in one simple philosophy: first make it work, then make it work better, a mindset I learned from my mentor, Rich Dudley. I thrive in environments where I can take a messy or incomplete problem and turn it into something reliable, scalable, and elegant.\nMy Projects \u0026amp; Work Highlights Platform Engineering \u0026amp; Architecture: Built and maintained Databricks environments in Azure using Terraform and Terragrunt. Configured Unity Catalogs, external locations, and volumes, and designed Dev, QA, and Prod environments. Contributed to cost optimization efforts in the Databricks Well-Architected Framework.\nData Pipelines \u0026amp; Medallion Architecture: Designed fault-tolerant pipelines that move data through the medallion architecture layers—bronze, silver, and gold—improving data quality at each step and preparing it for use by downstream teams. Pipelines were engineered to minimize manual steps, making workflows easy to deploy, manage, and maintain.\nSafeguarded Sensitive Procedures: Wrote code to handle sensitive processes mindfully, reducing the risk of mistakes. For example, I developed a WordPress ingestion client in Python with defensive safeguards to ensure proper usage.\nData Quality \u0026amp; Governance: Developed data quality checks and DLT pipelines using Data Expectations. Built dashboards to monitor data health, integrated governance tools like Atlan, and was selected as a Databricks Data \u0026amp; AI Summit speaker candidate, submitting a presentation on Data Expectations in Databricks.\nLarge Data Processing \u0026amp; Analytics: Tuned Spark pipelines and computed complex KPIs. Reverse-engineered Tally Street metrics—a tool used by accountants to extract KPIs from general ledgers—using Python and SQL to improve speed, accuracy, and accessibility.\nDemocratizing Data \u0026amp; AI: Built AI/BI Genies in Databricks, AI-driven tools that allow colleagues to ask questions about company data and generate dashboards, making AI and analytics accessible to non-technical users.\nSalesforce Knowledge Base Ingestion: Ingested Salesforce Knowledge Base articles—help desk pages—from Databricks using the Salesforce connector. Added this content to a vector index to power a search application, making it easier for customers to find support and internal teams to access knowledge.\nWordPress Website Ingestion: Created a Python connector from scratch to ingest the entire company WordPress site into Databricks. Implemented retry and backoff logic to handle API failures or network issues. Designed workflows to capture the full website on day 1, then only incremental changes on subsequent days—ensuring that temporary failures don’t require manual reruns and the system automatically catches up the next day.\nCI/CD \u0026amp; Automation: Configured service principals and created Azure DevOps CI/CD pipelines using Infrastructure as Code practices for reliable, repeatable deployments.\nDocumentation \u0026amp; Knowledge Sharing: Created Confluence documentation for all key processes, designed for future ingestion into a company-wide vector database to make knowledge accessible to other engineers.\nOutside the Data World When I’m not working with data, I like to stay active and explore the outdoors. Golfing and hiking are two of my favorite ways to recharge and stay focused, and I also love rock climbing, which keeps me on my toes—literally and figuratively!\nI’m also a huge pizza and coffee nerd:\nI’ve learned to make pizza dough from scratch, and true to my data-driven nature, I’ve kept a tally in 2025 of every pizza I’ve made. Nearly every morning, I make a pour-over coffee using my V60, my favorite coffee brewer. My go-to beans are typically light to medium roasts from George Howell Coffee. I also genuinely enjoy tinkering with personal coding projects, experimenting with new tools, and learning ways to make complex systems simpler and more efficient. And just like in my professional life, I love solving puzzles—whether it’s in code, a tricky climbing route, or perfecting a pizza crust!\nI’m always excited to connect with fellow data enthusiasts, share what I’ve learned, and continue growing as a data engineer. Thanks for stopping by my blog!\n","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eHi! I’m Matthew Norberg, a Data Engineer with a passion for turning complex data challenges into clean, maintainable, and high-performing solutions. Over the past several years, I’ve had the opportunity to work with Databricks, Azure, and a variety of modern data tools, building platforms, pipelines, and systems that help organizations make better use of their data.\u003c/p\u003e\n\u003ch2 id=\"my-journey-in-data-engineering\"\u003eMy Journey in Data Engineering\u003c/h2\u003e\n\u003cp\u003eI’ve always had a passion for programming and problem-solving, which first led me down the path of software engineering during my undergraduate studies. In graduate school, I started working more with data, earning a degree in Computer Science with a concentration in Data Science. This combination of software engineering and data expertise naturally led me to data engineering—a perfect middle ground between building robust systems and working with meaningful data. My background in CS and data science has prepared me well to tackle the challenges of modern data engineering.\u003c/p\u003e","title":"About Me"}]