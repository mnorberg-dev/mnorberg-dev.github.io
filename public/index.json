[{"content":"Databricks is quickly becoming one of the most popular data lakehouse platforms out there. Its popularity is growing fast, and for many developers, a significant portion of the job happens directly in the browser inside their Databricks workspace.\nYes, I know‚Äîthere are popular extensions that let you develop code outside of Databricks. Many folks swear by the VS Code extension because they prefer working in their favorite editor. But at the end of the day, you‚Äôre still going to spend time in the browser making sure your code runs as expected.\nThe Problem: Too Many Tabs, Too Few Clues Many organizations using Databricks have multiple workspaces to represent different environments‚Äîthink dev, qa, and prod. Some even have additional splits like staging or sandbox.\nIf you‚Äôre working across all of them, here‚Äôs the problem: your browser tabs all look the same.\nNote: Domains in images are redacted, but in your environment, they‚Äôll appear as GUIDs.\nInside the workspace, the situation isn‚Äôt much better. It\u0026rsquo;s not easy to determine which environment you are in. Technically, each environment does identify itself in two places though:\nThe domain name. Every environment has a URL like adb-\u0026lt;long string of numbers\u0026gt;. But let‚Äôs be honest‚Äînobody remembers arbitrary GUIDs. It‚Äôs the same reason DNS exists: humans prefer names like google.com instead of memorizing IP addresses. A small piece of text in the top-right corner. Sure, it‚Äôs there, but after seeing it hundreds of times, your brain starts ignoring it. It‚Äôs easy to miss, and if you‚Äôre juggling multiple tabs, you can‚Äôt even see that text without clicking into each one. The end result? Confusion, context-switching, and the very real risk of running a query in prod that you meant for dev.\nBut Wait, Isn‚Äôt There a Color Trick? Some people solve this by assigning different themes‚Äîsay, dark mode for prod and light mode for dev. Clever idea, but it falls apart quickly:\nThere are only two color schemes available and many organizations have three workspaces meaning one environment will inevitably match another. If two or more environments share a metastore, changing one changes the other. And let‚Äôs be honest: some developers simply refuse to use a theme they don‚Äôt like. So while the color trick can work in a pinch, it‚Äôs not a real solution.\nTo activate dark mode, navigate to Settings ‚Üí User Preferences ‚Üí ‚ÄúPrefer Dark.‚Äù\nIf you don‚Äôt enable this setting, your environment theme will default to match the light theme shown in the images above.\nThe Solution: Tampermonkey to the Rescue Enter Tampermonkey, a browser extension that lets you run custom JavaScript whenever you visit a matching URL.\nWith a short script, you can automatically label your Databricks browser tabs with an emoji + text identifier for each environment. Suddenly, dev, qa, and prod are crystal clear.\nIt‚Äôs simple, lightweight, and makes a huge difference:\nFaster navigation between environments Reduced risk of editing the wrong workspace A quality-of-life boost you‚Äôll wonder how you lived without And best of all: little to no performance impact. I‚Äôve been running this script for weeks, and my browser hasn‚Äôt skipped a beat. Below, I\u0026rsquo;ve provided an image illustrating what your dev environment will look like after setting up the Tamper Monkey script for yourself.\nHow to Set It Up Getting your Databricks tabs labeled automatically is quick and easy. You‚Äôll need to:\nInstall Tampermonkey Create a new script Paste in the code Configure your environment domains Save and enable the script Enable user scripts in your browser (Chrome only) Verify it‚Äôs working Note: These instructions are written for Google Chrome, but the steps can be adapted to other browsers that support Tampermonkey.\nFollow the steps below.\n1. Install Tampermonkey Download Tampermonkey for your browser from tampermonkey.net. Restart your browser after installation. 2. Create a New Script Open the Tampermonkey extension ‚Üí Dashboard ‚Üí ‚ÄúCreate a new script.‚Äù Delete the boilerplate code so the editor is blank. 3. Add the Script Copy and paste the following code into the new script:\n// ==UserScript== // @name Databricks Tab Emoji Label // @namespace http://tampermonkey.net/ // @version 1.5 // @description Add emoji and label to Databricks tab title based on environment // @author Matthew Norberg // @match https://\u0026lt;your-domain-here\u0026gt;.azuredatabricks.net/* // @grant none // ==/UserScript== (function() { \u0026#39;use strict\u0026#39;; function updateTitle() { const url = window.location.href; let label = \u0026#39;\u0026#39;; let emoji = \u0026#39;\u0026#39;; // Define your environment domains here let devDomain = \u0026#39;\u0026#39;; let qaDomain = \u0026#39;\u0026#39;; let prodDomain = \u0026#39;\u0026#39;; if (url.includes(devDomain)) { label = \u0026#39;DEV\u0026#39;; emoji = \u0026#39;üü¢\u0026#39;; } else if (url.includes(qaDomain)) { label = \u0026#39;QA\u0026#39;; emoji = \u0026#39;üü°\u0026#39;; } else if (url.includes(prodDomain)) { label = \u0026#39;PROD\u0026#39;; emoji = \u0026#39;üî¥\u0026#39;; } else { label = \u0026#39;OTHER\u0026#39;; emoji = \u0026#39;‚ö™\u0026#39;; } if (!document.title.startsWith(`[${emoji} ${label}]`)) { document.title = `[${emoji} ${label}] ${document.title}`; } } // Initial run setTimeout(updateTitle, 3000); // Monitor for URL changes in SPA let lastUrl = location.href; new MutationObserver(() =\u0026gt; { const currentUrl = location.href; if (currentUrl !== lastUrl) { lastUrl = currentUrl; setTimeout(updateTitle, 2000); } }).observe(document, {subtree: true, childList: true}); })(); 4. Configure Environment Domains Near the top of the script, replace the placeholders with your actual environment domains:\nlet devDomain = \u0026#39;your-dev-domain\u0026#39;; let qaDomain = \u0026#39;your-qa-domain\u0026#39;; let prodDomain = \u0026#39;your-prod-domain\u0026#39;; 5. Save and Enable the Script Save your changes (File ‚Üí Save).\nMake sure the script toggle in Tampermonkey is turned on (green).\n6. Enable User Scripts in Chrome Go to chrome://extensions ‚Üí find Tampermonkey ‚Üí click Details.\nMake sure Allow user scripts is enabled.\nNote: Other browsers may have different settings for allowing user scripts‚Äîadapt accordingly.\n7. Verify installation Open your Databricks environment in a new tab.\nAfter a few seconds, your tab title should display the correct emoji + environment label.\nTroubleshooting Tab title doesn‚Äôt update right away\nThe script intentionally waits 3 seconds after page load before applying changes.\nWithout this delay, the title gets updated but is overwritten during the web page loading process.\nIn testing, 3 seconds worked well, but you can adjust this by editing the setTimeout call in the code:\nsetTimeout(updateTitle, 3000); Still not working?\nDouble-check that:\nThe script is enabled in Tampermonkey. Your domains are correctly set in both the variables and the @match lines. ‚ÄúAllow user scripts‚Äù is enabled in your browser‚Äôs extension settings. Final Thoughts It‚Äôs a small tweak, but it solves a surprisingly big problem. If you‚Äôre juggling multiple Databricks environments, this little script will save you from confusion‚Äîand maybe even prevent a mistake or two.\nSometimes the best tools aren‚Äôt the big, complicated ones. They‚Äôre the tiny hacks that make your day smoother.\n","permalink":"https://mnorberg-dev.github.io/posts/databricks-tab-label-tool/","summary":"\u003cp\u003eDatabricks is quickly becoming one of the most popular data lakehouse platforms out there. Its popularity is growing fast, and for many developers, a significant portion of the job happens directly in the browser inside their Databricks workspace.\u003c/p\u003e\n\u003cp\u003eYes, I know‚Äîthere are popular extensions that let you develop code outside of Databricks. Many folks swear by the VS Code extension because they prefer working in their favorite editor. But at the end of the day, you‚Äôre still going to spend time in the browser making sure your code runs as expected.\u003c/p\u003e","title":"The Databricks Tool You Didn't Know You Needed"},{"content":"Hi! I‚Äôm Matthew Norberg, a Data Engineer with a passion for turning complex data challenges into clean, maintainable, and high-performing solutions. Over the past several years, I‚Äôve had the opportunity to work with Databricks, Azure, and a variety of modern data tools, building platforms, pipelines, and systems that help organizations make better use of their data.\nMy Journey in Data Engineering I‚Äôve always had a passion for programming and problem-solving, which first led me down the path of software engineering during my undergraduate studies. In graduate school, I started working more with data, earning a degree in Computer Science with a concentration in Data Science. This combination of software engineering and data expertise naturally led me to data engineering‚Äîa perfect middle ground between building robust systems and working with meaningful data. My background in CS and data science has prepared me well to tackle the challenges of modern data engineering.\nMy approach to data engineering can be summed up in one simple philosophy: first make it work, then make it work better, a mindset I learned from my mentor, Rich Dudley. I thrive in environments where I can take a messy or incomplete problem and turn it into something reliable, scalable, and elegant.\nMy Projects \u0026amp; Work Highlights Platform Engineering \u0026amp; Architecture: Built and maintained Databricks environments in Azure using Terraform and Terragrunt. Configured Unity Catalogs, external locations, and volumes, and designed Dev, QA, and Prod environments. Contributed to cost optimization efforts in the Databricks Well-Architected Framework.\nData Pipelines \u0026amp; Medallion Architecture: Designed fault-tolerant pipelines that move data through the medallion architecture layers‚Äîbronze, silver, and gold‚Äîimproving data quality at each step and preparing it for use by downstream teams. Pipelines were engineered to minimize manual steps, making workflows easy to deploy, manage, and maintain.\nSafeguarded Sensitive Procedures: Wrote code to handle sensitive processes mindfully, reducing the risk of mistakes. For example, I developed a WordPress ingestion client in Python with defensive safeguards to ensure proper usage.\nData Quality \u0026amp; Governance: Developed data quality checks and DLT pipelines using Data Expectations. Built dashboards to monitor data health, integrated governance tools like Atlan, and was selected as a Databricks Data \u0026amp; AI Summit speaker candidate, submitting a presentation on Data Expectations in Databricks.\nLarge Data Processing \u0026amp; Analytics: Tuned Spark pipelines and computed complex KPIs. Reverse-engineered Tally Street metrics‚Äîa tool used by accountants to extract KPIs from general ledgers‚Äîusing Python and SQL to improve speed, accuracy, and accessibility.\nDemocratizing Data \u0026amp; AI: Built AI/BI Genies in Databricks, AI-driven tools that allow colleagues to ask questions about company data and generate dashboards, making AI and analytics accessible to non-technical users.\nSalesforce Knowledge Base Ingestion: Ingested Salesforce Knowledge Base articles‚Äîhelp desk pages‚Äîfrom Databricks using the Salesforce connector. Added this content to a vector index to power a search application, making it easier for customers to find support and internal teams to access knowledge.\nWordPress Website Ingestion: Created a Python connector from scratch to ingest the entire company WordPress site into Databricks. Implemented retry and backoff logic to handle API failures or network issues. Designed workflows to capture the full website on day 1, then only incremental changes on subsequent days‚Äîensuring that temporary failures don‚Äôt require manual reruns and the system automatically catches up the next day.\nCI/CD \u0026amp; Automation: Configured service principals and created Azure DevOps CI/CD pipelines using Infrastructure as Code practices for reliable, repeatable deployments.\nDocumentation \u0026amp; Knowledge Sharing: Created Confluence documentation for all key processes, designed for future ingestion into a company-wide vector database to make knowledge accessible to other engineers.\nOutside the Data World When I‚Äôm not working with data, I like to stay active and explore the outdoors. Golfing and hiking are two of my favorite ways to recharge and stay focused, and I also love rock climbing, which keeps me on my toes‚Äîliterally and figuratively!\nI‚Äôm also a huge pizza and coffee nerd:\nI‚Äôve learned to make pizza dough from scratch, and true to my data-driven nature, I‚Äôve kept a tally in 2025 of every pizza I‚Äôve made. Nearly every morning, I make a pour-over coffee using my V60, my favorite coffee brewer. My go-to beans are typically light to medium roasts from George Howell Coffee. I also genuinely enjoy tinkering with personal coding projects, experimenting with new tools, and learning ways to make complex systems simpler and more efficient. And just like in my professional life, I love solving puzzles‚Äîwhether it‚Äôs in code, a tricky climbing route, or perfecting a pizza crust!\nI‚Äôm always excited to connect with fellow data enthusiasts, share what I‚Äôve learned, and continue growing as a data engineer. Thanks for stopping by my blog!\n","permalink":"https://mnorberg-dev.github.io/about/","summary":"\u003cp\u003eHi! I‚Äôm Matthew Norberg, a Data Engineer with a passion for turning complex data challenges into clean, maintainable, and high-performing solutions. Over the past several years, I‚Äôve had the opportunity to work with Databricks, Azure, and a variety of modern data tools, building platforms, pipelines, and systems that help organizations make better use of their data.\u003c/p\u003e\n\u003ch2 id=\"my-journey-in-data-engineering\"\u003eMy Journey in Data Engineering\u003c/h2\u003e\n\u003cp\u003eI‚Äôve always had a passion for programming and problem-solving, which first led me down the path of software engineering during my undergraduate studies. In graduate school, I started working more with data, earning a degree in Computer Science with a concentration in Data Science. This combination of software engineering and data expertise naturally led me to data engineering‚Äîa perfect middle ground between building robust systems and working with meaningful data. My background in CS and data science has prepared me well to tackle the challenges of modern data engineering.\u003c/p\u003e","title":"About Me"}]